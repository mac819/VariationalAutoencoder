{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070d6411",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-Dataset\" data-toc-modified-id=\"Loading-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading Dataset</a></span></li><li><span><a href=\"#Reading-TFRecord-Data\" data-toc-modified-id=\"Reading-TFRecord-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reading TFRecord Data</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from vae.config import *\n",
    "from vae.data_processing import read_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Numpy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17440330",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = MONET_TFREC_PATH + \"/*.tfrec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a576693",
   "metadata": {},
   "outputs": [],
   "source": [
    "monet_file_path = glob.glob(MONET_TFREC_PATH + \"/*.tfrec\")\n",
    "photo_file_path = glob.glob(PHOTO_TFREC_PATH + \"/*.tfrec\")\n",
    "print(len(monet_file_path), len(photo_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcc44a",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_dataset = tf.data.TFRecordDataset(photo_file_path)\n",
    "monet_dataset = tf.data.TFRecordDataset(monet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c0d2c",
   "metadata": {},
   "source": [
    "# Reading TFRecord Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_monet_dataset = monet_dataset.map(read_tfrecord)\n",
    "parsed_photo_dataset = photo_dataset.map(read_tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_image(data):\n",
    "    return data/255\n",
    "\n",
    "scaled_monet_dataset = parsed_monet_dataset.map(scale_image)\n",
    "scaled_photo_dataset = parsed_photo_dataset.map(scale_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158601fe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2bf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input\n",
    "from vae.model import  encoder, \\\n",
    "                        decoder, \\\n",
    "                        kl_loss, \\\n",
    "                        mse_loss, \\\n",
    "                        vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d581b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "IMAGE_DEPTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af213d14",
   "metadata": {},
   "source": [
    "**Building Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_inputs= Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH,IMAGE_DEPTH))\n",
    "latent_vec, mean, log_var  = encoder(vae_inputs)\n",
    "recons_image = decoder(latent_vec)\n",
    "\n",
    "vae_model = keras.Model(inputs=vae_inputs, outputs=[recons_image, mean, log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54162aac",
   "metadata": {},
   "source": [
    "**Training Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "epoch = 5\n",
    "\n",
    "for i in range(epoch):\n",
    "    for train_data in scaled_monet_dataset.batch(32):\n",
    "\n",
    "        # Forward Pass\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Froward Pass\n",
    "            recons_img, mean, log_var = vae_model(train_data)\n",
    "            # Loss\n",
    "            model_loss = vae_loss(train_data, recons_img, mean, log_var)\n",
    "\n",
    "        gradient = tape.gradient(model_loss, vae_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradient, vae_model.trainable_weights))\n",
    "    print(f\"Epoch: {i} --- Loss Value: {tf.reduce_sum(model_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f66ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Trained Model\n",
    "vae_model.save(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laoding Saved Models\n",
    "vae_model = keras.models.load_model(\"saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch_list = []\n",
    "for batch in scaled_monet_dataset.take(100).shuffle(101).batch(25):\n",
    "    img_batch_list.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddaf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img_batch):\n",
    "\n",
    "    figsize = 15\n",
    "    fig = plt.figure(figsize=(figsize, 10))\n",
    "\n",
    "    for i in range(25):\n",
    "        ax = fig.add_subplot(5, 5, i+1)\n",
    "        ax.axis('off')\n",
    "        img = img_batch[i]\n",
    "        ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img_batch_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(vae_model.predict(img_batch_list[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acb6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38e3c154",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffe0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae.model import Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Encoder()\n",
    "decoder_model = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataset):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b208a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86dfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f0801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
